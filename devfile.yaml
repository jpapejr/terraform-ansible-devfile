schemaVersion: 2.2.0
metadata:
  name: terraform-ansible-dev
  version: 1.0.0
  description: Workspace for Terraform and Ansible development with LLM capabilities
components:
  - name: tools
    container:
      image: registry.access.redhat.com/ubi9/ubi-minimal:latest
      command: ["sleep", "infinity"]
      memoryLimit: 2Gi
      env:
        - name: ANSIBLE_CONFIG
          value: /home/user/.ansible.cfg
      volumeMounts:
        - name: ansible-config
          path: /home/user/.ansible.cfg
  - name: vscode-theia
    editor:
      id: che-incubator/che-code/insiders
    extensions:
      - redhat.ansible
      - ms-azuretools.vscode-azureterraform
      - continue.continue
  - name: ollama
    container:
      image: ollama/ollama:latest
      ports:
        - name: ollama-api
          containerPort: 11434
          protocol: TCP
      command: ["ollama", "serve"]
      memoryLimit: 4Gi
      volumes:
        - name: ollama-data
          mountPath: /root/.ollama
      env:
        - name: OLLAMA_HOST
          value: 0.0.0.0
        - name: OLLAMA_LISTEN
          value: ":11434"
  commands:
    - id: terraform-init
      exec:
        component: tools
        commandLine: terraform init
        workingDir: ${PROJECT_SOURCE}
    - id: terraform-plan
      exec:
        component: tools
        commandLine: terraform plan
        workingDir: ${PROJECT_SOURCE}
    - id: terraform-apply
      exec:
        component: tools
        commandLine: terraform apply
        workingDir: ${PROJECT_SOURCE}
    - id: ansible-lint
      exec:
        component: tools
        commandLine: ansible-lint --version
        workingDir: ${PROJECT_SOURCE}
    - id: ansible-playbook-syntax-check
      exec:
        component: tools
        commandLine: ansible-playbook --syntax-check playbook.yml
        workingDir: ${PROJECT_SOURCE}
    - id: ansible-run
      exec:
        component: tools
        commandLine: ansible-playbook -i inventory.ini playbook.yml
        workingDir: ${PROJECT_SOURCE}
    - id: ollama-pull-model
      description: Pull an LLM model
      args:
        - name: model
          type: string
          default: llama2
          description: Model to pull (e.g., llama2, mistral)
      exec:
        component: ollama
        commandLine: ollama pull ${model}
    - id: ollama-run
      description: Run a query against a model
      args:
        - name: model
          type: string
          default: llama2
          description: Model to use
        - name: prompt
          type: string
          description: Query to send to the model
      exec:
        component: ollama
        commandLine: ollama run ${model} "${prompt}"
volumes:
  - name: ansible-config
    container:
      path: /home/user/.ansible.cfg
